# -*- coding: utf-8 -*-
"""RNN English to Spanish Translator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NXYl83-N8njrwHhx7wXyJbg2Kjn6YPwh
"""

# Zeynep Ozdol
# ITP 259 Spring 2025
# RNN English to Spanish translator

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dropout, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
import tensorflow as tf

# Process the dataset into English and Spanish sentences
df = pd.read_csv('sample_data/english-spanish-dataset.csv')
df = df[['english', 'spanish']]
df = df.dropna().iloc[:50000]

english_sentences = df['english'].astype(str).values
spanish_sentences = df['spanish'].astype(str).values

# Tokenize the sentences in both languages
eng_tokenizer = Tokenizer()
eng_tokenizer.fit_on_texts(english_sentences)
eng_sequences = eng_tokenizer.texts_to_sequences(english_sentences)
eng_word_index = eng_tokenizer.word_index
eng_vocab_size = len(eng_word_index) + 1

spa_tokenizer = Tokenizer()
spa_tokenizer.fit_on_texts(spanish_sentences)
spa_sequences = spa_tokenizer.texts_to_sequences(spanish_sentences)
spa_word_index = spa_tokenizer.word_index
spa_vocab_size = len(spa_word_index) + 1

# Pad the sentences
max_seq_length = max(
    max(len(seq) for seq in eng_sequences),
    max(len(seq) for seq in spa_sequences))

X = pad_sequences(eng_sequences, maxlen=max_seq_length, padding='post')
y = pad_sequences(spa_sequences, maxlen=max_seq_length, padding='post')

# Build a RNN model
model = Sequential()
model.add(Embedding(input_dim=eng_vocab_size, output_dim=128, input_length=max_eng_len))
model.add(SimpleRNN(128, return_sequences=True))
model.add(Dropout(0.5))
model.add(Dense(spa_vocab_size, activation='softmax'))

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2023)
print(X_train.shape)
print(y_train.shape)
history = model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))

plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.title("Loss Curve") # Loss curve

plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.legend()
plt.title("Accuracy Curve") # Accuracy curve

plt.show()

# Prompt the user to enter an English sentence
def translate_sentence(sentence):
    seq = eng_tokenizer.texts_to_sequences([sentence])
    padded = pad_sequences(seq, maxlen=max_eng_len, padding='post')
    pred = model.predict(padded)
    pred_indices = np.argmax(pred[0], axis=1)
    predicted_words = [list(spa_word_index.keys())[list(spa_word_index.values()).index(idx)]
                       if idx in spa_word_index.values() else ''
                       for idx in pred_indices]
    return ' '.join(predicted_words)

user_input = "this german hotel belongs to the company"
print("Translated:", translate_sentence(user_input))