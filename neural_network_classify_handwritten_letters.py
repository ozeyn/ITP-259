# -*- coding: utf-8 -*-
"""neural network classify handwritten letters.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/156UIDmcgMwjFC5g3cwEhE0ddi0WEw2BO
"""

# Zeynep Ozdol
# ITP 259 Spring 2025
# Design and train a neural network to classify handwritten Latin letters A-Z

import random
import pandas as pd
import seaborn as sb
import numpy as np
import matplotlib.pyplot as plt
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

df = pd.read_csv("sample_data/A_Z Handwritten Data.csv") # Read the dataset into a dataframe
df = df.fillna(0)

df.head()

X = df.drop('label', axis=1) # Feature set
y = df['label'] # Target variable

print("Shape of feature set (X):", X.shape)
print("Shape of target variable (y):", y.shape)

word_dict = {0:'A', 1:'B', 2:'C', 3:'D', 4:'E', 5:'F', 6:'G',
             7:'H', 8:'I', 9:'J', 10:'K', 11:'L', 12:'M',
             13:'N', 14:'O', 15:'P', 16:'Q', 17:'R', 18:'S',
             19:'T', 20:'U', 21:'V', 22:'W', 23:'X', 24:'Y', 25:'Z'} # Map numbers to letters

df['letter'] = df['label'].map(word_dict) # Print the dictionary

# Plot the histogram
plt.figure(figsize=(12, 6))
sb.countplot(x='letter', data=df)
plt.xlabel("Label")
plt.ylabel("Count")
plt.xticks(rotation=0)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

# Display 64 random letters from the dataset with their labels
plt.figure(figsize=(10, 10))

random_samples = df.sample(64)

for i, (index, row) in enumerate(random_samples.iterrows()):
    plt.subplot(8, 8, i + 1)
    img_data = row.drop(['label', 'letter']).values.astype(float).reshape(28, 28)
    plt.imshow(img_data, cmap='gray')
    plt.title(f"{row['letter']}")
    plt.axis('off')

plt.tight_layout()
plt.show()

# Partition the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2023, stratify=y)

# Print the shapes of the train and test sets to verify the split
print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)

X_train_scaled = X_train/255
X_test_scaled = X_test/255 # Scale

mlp = MLPClassifier(hidden_layer_sizes=(200, 200, 200), activation="relu",
                    max_iter=25, alpha=1e-3, solver="adam",
                    random_state=2022, learning_rate_init=0.01, verbose=True) # Create an MLP classifier
mlp.fit(X_train_scaled, y_train)

y_pred = mlp.predict(X_test_scaled)
plt.plot(mlp.loss_curve_)
plt.show() # Loss curve

print("The accuracy is", mlp.score(X_test_scaled,y_test))

y_pred = mlp.predict(X_test.values)
cm = confusion_matrix(y_pred, y_test)
ConfusionMatrixDisplay(confusion_matrix=cm).plot()
plt.show() # Confusion matrix

# Display predicted and actual letter of the first row in the test dataset
test_sample = np.array(X_test.iloc[0]).reshape(28, 28)
plt.imshow(test_sample, cmap="gray")
plt.title("The predicted digit is "+ str(y_pred[0]) + ". The human label was " + str(y_test[0]))
plt.show()

# Display predicted and actual letter of a misclassified letter
misclassified_idx = np.where(y_pred != y_test)[0][0]
plt.imshow(X_test.iloc[misclassified_idx].values.reshape(28, 28), cmap="gray")
plt.title(f"Predicted: {word_dict[y_pred[misclassified_idx]]}, Actual: {word_dict[y_test.iloc[misclassified_idx]]}")
plt.show()